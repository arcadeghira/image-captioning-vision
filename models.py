"""Custom implementation of J. Lu et al., Knowing when to Look: Adaptive Attention via a Visual Sentinel for Image Captioning, 2017
https://arxiv.org/pdf/1612.01887.pdf"""

# PyTorch modules
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

from torch.autograd import Variable
from torch.nn import init
from torch.nn.utils.rnn import pack_padded_sequence

# Note: Every time you will see the dimension B it should be interpreted as COCO's batch size.

class AttentiveCNN(nn.Module):
    """Module that implements the CNN encoder, which has to transform COCO's input images in features. As underlined in the paper, we should use a pre-trained model specifically able to identify convolutional features: here, ResNet152 is chosen."""

    def __init__(self, embed_size, hidden_size):
        """Since ResNet152 is meant to perform object detection, we need to take out its last two layers (i.e., fully connected and average pooling) because we are instead interested at the last featural representation generated by ResNet itself, as the authors suggested.
        
        embed_size : int
            Embedding space size in which visual features (V and v_g) are mapped.
            Note: Designed to be half of d := 256.
            
        hidden_size : int
            Hidden space size of LSTM's hidden state.
            Note: Referenced in the paper as d := 512    
        """
        super(AttentiveCNN, self).__init__()
        
        # Load the pre-trained ResNet152 model
        resnet = models.resnet152(pretrained=True)

        # https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/3
        # children() function returns the layers of the model.
        modules = list(resnet.children())[:-2] # Delete the last two layers
        
        # Generate a new Sequential model with all ResNet152 modules, bar the last two. By doing so we gain access to those modules, and can actually modify the model to introduce the layers we need to embed ResNet's last convolutional features into the hidden space of V - visual features - and the embedding space of v_g - global visual feature.
        resnet_conv = nn.Sequential(*modules)
        
        # Instatiate attributes
        self.resnet_conv = resnet_conv
        self.avgpool = nn.AvgPool2d(7) # ResNet features have size 2048 x 7 x 7,
                                       # hence the 7 in the 2D average pooling

        """Generate two Linear modules (in_features: Input sample size, out_features: Output sample size) compliant with the linear combinations described in the paper to generate the visual features, V and v_g, starting from the image features, A and a_g. a_g represents the global image feature, obtained with a 2D average pooling on A.
        Refer to forward() for a more in-depth explanation of the whole procedure.
        
        affine_a : Eq. (15) from the paper
            V = ReLU(W_a * A), or, indivually, v_i = ReLU(W_a * a_i)

        affine_b : Eq. (16) from the paper
            v_g = ReLU(W_b * a_g)

        Our KwtL model, through these two Linear modules, will be able to learn the weights W_a and W_b, as per the paper's requirement.
        """

        # 2048 is the dimensionality of ResNet features
        self.affine_a = nn.Linear(2048, hidden_size)
        self.affine_b = nn.Linear(2048, embed_size)
        
        # Initialize the above modules
        self.init_weights()

        # Dropout before affine transformation
        self.dropout = nn.Dropout(0.5)
        
    def init_weights(self):
        """Initialize weights and biases of the two Linear modules. To do the initialization we follow the Kaiming uniform distribution described in the paper 'Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification.', as it was suggested in many articles, and put to 0 all biases."""

        # Initialize weights
        init.kaiming_uniform_(self.affine_a.weight, mode='fan_in')
        init.kaiming_uniform_(self.affine_b.weight, mode='fan_in')

        # Initialize biases
        self.affine_a.bias.data.fill_(0)
        self.affine_b.bias.data.fill_(0)
        
    def forward(self, images):
        """Compute visual features from the outputs of the ResNet152 model on COCO's images.
        
        Parameters
        ----------
        images : torch.Tensor (B x 3 x 224 x 224)
            Batch of COCO's RGB images

        Returns
        -------
        V : torch.Tensor (B x k x hidden_size)
            Visual features [v_1, v_2, ..., v_k]
            Note: k here is 49, as it is a 7 x 7 ResNet feature unrolled

        v_g : torch.Tensor (B x embed_size)
            Global visual feature
        """
        
        """
        A : torch.Tensor (B x 2048 x 7 x 7)
            Convolutional image features
        """
        # Apply the pre-trained ResNet152 model (bar last two layers) on COCO's images.
        A = self.resnet_conv(images)
        
        """
        a_g : torch.Tensor (B x 2048 x 1)
            2D average pooling feature map

        Compute the global descriptor of each image through 2D average pooling on ResNet's outputs. The kernel properties (i.e., stride, padding, etc...) are kept as default, whereas its size has been previously set to 7. We hypothesize that the dimensions contained in paper are reported with respect to the Torch implementation of neural networks, which is also available in the LUA programming language. If that's the case the 2D averaging is then performed on each 7 x 7 slice, yielding an output Torch tensor of 2048.
        """
        a_g = self.avgpool(A)

        """PyTorch allows a tensor to be a View object of an existing tensor, that is, it will share they same underlying data with its base tensor without being an explicit copy, thus allowing to do fast and memory efficient reshaping, slicing and elemen-wise operations.
        
        a_g.size(0) is equal to the batch size B

        a_g.view(..., -1) with the -1 option allows to infer the leftover dimension from the previous dimensions of the tensor, effectively unrolling them into a single dimension. Hence, since the leftover dimensions were 1 and 2048, the unrolled dimension will be 1*2048 = 2048.
        """
        a_g = a_g.view(a_g.size(0), -1) # B x 2048

        """Citing the paper verbatim: "We want to embed the image features A (and a_g) from R^2048 to R^d (and R^{embed_size}). In order to that we will use a single layer perceptron with ReLU as the activation function of choice".
        
        Here me move A's dimensions around through the View object, to make its dimensions compliant with Torch requirements:
            - A.view(A.size(0), A.size(1), -1) : (B x 2048 x k)
            - A....transpose(1, 2) : (B x k x 2048)
            - V = F.relu... : (B x k x hidden_size)

        This follows V being equal to [v_1, v_2, ..., v_k], with every v_i embedded into R^d.
        """
        V = A.view(A.size(0), A.size(1), -1).transpose(1, 2)
        V = F.relu(self.affine_a(self.dropout(V)))
        
        # Global visual feature
        v_g = F.relu(self.affine_b(self.dropout(a_g))) # B x embed_size
        
        return V, v_g

class Atten(nn.Module):
    """Attention block that, given a sentinel vector s_t, will first compute the fully spatial context vector c_t and then the one proposed in the KwtL paper, c_hat_t, which is a context vector that comprises information both from the LSTM's hidden state (embedded in s_t) and the spatial image features V returned by the encoder."""

    def __init__(self, hidden_size):
        """Define a few Linear modules that map to the weighted dot products needed to compute the attention scores alpha_hat_t.

        All those affines map transformations from Eq. (12) from the paper, which is an extension of Eq. (6) that computed attention scores solely on spatial information. They all have to be learned by the KwtL model during training. The first three (capital W) are embedded in a R^{k x d} space, because they are going to take an input of hidden size from the LSTM itself and will rank all the k attention regions with a score.
        
        affine_v := W_v in R^{k x d}
            Weight matrix to attend from spatial features V

        affine_g := W_g in R^{k x d}
            Weight matrix to attend from the LSTM's hidden state h_t      
        
        affine_s := W_s in R^{k x d}
            Weight matrix to attend from the sentinel vector s_t (that is, in a certain way, what the LSTM already knows about what's been generated, which can greatly help determine the most suitable region to attend next out of the available k).
            
        affine_h := w_h in R^{k}
            Weight vector that collates all the above into k scores
        """
        super(Atten, self).__init__()

        self.affine_h = nn.Linear(49, 1, bias=False) # w_h
        self.affine_v = nn.Linear(hidden_size, 49, bias=False) # W_v
        self.affine_g = nn.Linear(hidden_size, 49, bias=False) # W_g
        self.affine_s = nn.Linear(hidden_size, 49, bias=False) # W_s
        
        self.dropout = nn.Dropout(0.5)
        self.init_weights()
        
    def init_weights(self):
        """Initialize weights of the four Linear modules. To do the initialization we follow the Xavier uniform distribution described in the paper 'Understanding the difficulty 
        of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)', as it was suggested in many articles."""

        init.xavier_uniform_(self.affine_v.weight)
        init.xavier_uniform_(self.affine_g.weight)
        init.xavier_uniform_(self.affine_h.weight)
        init.xavier_uniform_(self.affine_s.weight)
        
    def forward(self, V, h_t, s_t):
        """
        Compute the overall context vector c_hat_t given the spatial image features V, the hidden state h_t and the sentinel vector s_t. It's going to apply the affines mapping to the weighted transformations, following Eq. (6), (7) and (12) from the paper.
        
        First we are going to compute the fully spatial attention scores alpha_t in R^{k} in order to compute the fully spatial context vector c_t. Then we will concat the unnormalized fully spatial attention scores z_t in R^{k} with the single score coming out of the sentinel vector in Eq. (12). This will allow us to compute a final vector of attention scores alpha_hat_t in R^{k + 1} whose last entry is identified as beta_t, the sentinel gate which regulates through Eq. (11) the final context vector c_hat_t by choosing how much influence should be posed on the sentinel vector (what the LSTM already knows) rather than on the fully spatial context vector (coming out of V).
        """

        # Eq. (6.a) : W_v * V + W_g * h_t * 1^T
        # (B x k x k)
        content_V = self.affine_v(self.dropout(V)).unsqueeze(1) \
                        + self.affine_g(self.dropout(h_t)).unsqueeze(2)
        
        # Eq. (6.b) : z_t = W_h * tanh(content_V)
        # (B x 1 x k)
        z_t = self.affine_h(self.dropout(F.tanh(content_V))).squeeze(3)

        # Eq. (7) : alpha_t = softmax(z_t)
        # Compute the softmax on the unrolled 2D vector, then in 3D.
        # (B x k) --> (B x 1 x k)
        alpha_t = F.softmax(z_t.view(-1, z_t.size(2))) \
                   .view(z_t.size(0), z_t.size(1), -1)

        """
        The following formula is used to obtain the context vector computed when considering both spatial image features, V, and the current hidden state of a LSTM. It simply computes the linear sum of all the vectors v_1, ..., v_k with the corresponding vector of alphas. Each alpha weights a slice of the features, therefore the two vectors alpha_t and V must have equal size (B x k x hidden_size), giving as output c_t.
        
        c_t : torch.Tensor (B x k x hidden_size)
            Fully spatial context vector
        """
        # Eq. (8) : \sum_{i = 1}^{k} alpha_ti c_ti
        c_t = torch.bmm(alpha_t, V).squeeze(2)

        """
        The following two formulas extend Eq. (6) into (12) to allow the extraction of content from the sentinel vector s_t.

        z_t_extended : torch.Tensor of 1 element
            Rightmost score to be concatenated to z_t
        """
        
        # Eq. (12.a) : W_s * s_t + W_g * h_t
        content_s = self.affine_s(self.dropout(s_t)) + self.affine_g(self.dropout(h_t))

        # Eq. (12.b) : w_t * tanh(content_s)
        z_t_extended = self.affine_h(self.dropout(F.tanh(content_s)))
        extended = torch.cat((z_t, z_t_extended), dim=2)

        # Eq. (12.c) : alpha_hat_t = softmax(extended)
        alpha_hat_t = F.softmax(extended.view(-1, extended.size(2))) \
                       .view(extended.size(0), extended.size(1), -1)

        # The last element of alpha_hat_t is the sentinel gate value,
        # as described in the paper
        beta_t = alpha_hat_t[:, :, -1]
        beta_t = beta_t.unsqueeze(2)

        """Compute the final context vector c_hat_t"""
        
        # Eq. (11) : c_hat_t = beta * s_t + (1 - beta) * c_t
        c_hat_t = beta_t * s_t + (1 - beta_t) * c_t

        return c_hat_t, alpha_t, beta_t

class Sentinel(nn.Module):
    """
    Sentinel block that extends the classic LSTM by adding up the sentinel gate which regulates a visual sentinel vector s_t. This vector is computed from the current input x_t, which we recall concatenates the word embeddings vector w_t with the global visual feature v_g, and the previous step's hidden and memory's cell states, h_{t - 1} and m_{t - 1}, to give a perception of what the LSTM already knows about what's been generated before (which of the k regions have already been attended).
    """

    def __init__(self, input_size, hidden_size):
        super(Sentinel, self).__init__()

        # Linear modules needed to compute the memory gate g_t (Eq. (9))
        self.affine_x = nn.Linear(input_size, hidden_size, bias=False)  # W_x
        self.affine_h = nn.Linear(hidden_size, hidden_size, bias=False) # W_h
        
        # Dropout applied before affine transformation
        self.dropout = nn.Dropout(0.5)
        
        self.init_weights()
        
    def init_weights(self):
        init.xavier_uniform_(self.affine_x.weight)
        init.xavier_uniform_(self.affine_h.weight)
        
    def forward(self, x_t, h_t_1, cell_t):
        """Implement Eq. (9) and (10) from the paper
        
        g_t, h_t, s_t : torch.Tensor (B x hidden_size)
            Vectors of interest
        """

        # Eq. (9) : g_t = sigmoid(W_x * x_t + W_h * h_(t-1))
        g_t = self.affine_x(self.dropout(x_t)) \
                    + self.affine_h(self.dropout(h_t_1))
        g_t = F.sigmoid(g_t)
        
        # Eq. (10) : s_t = g_t * tanh(m_t)
        # Note: * here stands for element-wise product
        s_t = g_t * F.tanh(cell_t)
        
        return s_t

class AdaptiveBlock(nn.Module):
    """
    Adaptive block that given a Sentinel and an Atten object, and knowing the dimensions of the embedding space, d / 2, the hidden space, d, and the vocabulary, J, is able to produce the output scores (likelihood of every word to be the correct one) for a given input through a multi-layer perceptron (MLP) that maps from d to J.
    """
    
    def __init__(self, embed_size, hidden_size, vocab_size):
        super(AdaptiveBlock, self).__init__()
        
        """
        Definition of individual blocks that compose the AdaptiveBlock object.

        Note: self.sentinel takes embed_size * 2 as input size for x_t, because both the word embeddings, w_t, and the global visual feature, v_g, are of size embed_size and, when concating along that dimension, they form an input x_t exactly of size embed_size * 2. Refer to the paper when they mention the dimension d.
        """
        # Sentinel block
        self.sentinel = Sentinel(embed_size * 2, hidden_size)
        # Final attention block
        self.atten = Atten(hidden_size)
        # Final caption generator
        self.mlp = nn.Linear(hidden_size, vocab_size)
        
        # Dropout layer inside affine transformation
        self.dropout = nn.Dropout(0.5)
        
        # Initialize variables
        self.hidden_size = hidden_size
        self.init_weights()
        
    def init_weights(self):
        """
        Initialize final MLP classifier weights and biases
        """
        init.kaiming_normal_(self.mlp.weight, mode='fan_in')
        self.mlp.bias.data.fill_(0)
        
    def forward(self, x, hiddens, cells, V):
        """Implement Eq. (13) in the paper to output log-likelihood scores for each word in the vocabulary.

        Parameters
        ----------
        x : torch.Tensor (B x hidden_size)
            Input to the LSTM

        hiddens : torch.Tensor
            Registered hidden states (may be empty)

        cells : torch.Tensor
            Registered memory cell states

        V : torch.Tensor (B x k x hidden_size)
            Visual features from CNN encoder

        Returns
        -------
        scores : torch.Tensor (B x vocab_size)
            Log-likelihood scores for each word in the vocabulary

        atten_weights : torch.Tensor (B x 1 x k)
            Fully spatial attention scores (Eq. (7))

        beta : torch.Tensor (B x 1)
            Sentinel gate value.

            Note: It is defined as the complementary of the visual grounding probability, that is, the probability that a given word in the vocabulary would be taken into consideration for a generic region of an image to thoroughly describe it. beta := 1 - vg_prob.
        """
        
        # Initialize the hidden state h_0 and verify whether more hidden states are being calculated
        # as the sentinel vector s_t requires the use of the previous hidden state h_(t - 1).
        h_0 = self.init_hidden(x.size(0))[0].transpose(0, 1) # B x 1 x hidden_size

        # Concatenate h_0 up-front to hiddens without the first entry if there are more than one hidden states stored, otherwise just initialize the new hiddens vector with the first hidden state that has just been initialized above. h_(t - 1): B x k x hidden_size
        if hiddens.size(1) > 1:
            hiddens_t_1 = torch.cat((h_0, hiddens[:, :-1, :]), dim=1)
        else:
            hiddens_t_1 = h_0

        # Get sentinel vector s_t
        sentinel = self.sentinel(x, hiddens_t_1, cells)
        
        # Get context vector c_hat_t, spatial attention
        # weights alpha_t and sentinel gate beta_t
        c_hat, atten_weights, beta = self.atten(V, hiddens, sentinel)
        
        # Eq. (13) : Final scores across vocabulary.
        # Note: Score != probability, as no softmax is applied here
        scores = self.mlp(self.dropout(c_hat + hiddens))
        
        return scores, atten_weights, beta
    
    def init_hidden(self, batch_size):
        """Initialize to 0 the first hidden and memory cell states, h_0 and m_0, given the batch size B."""

        # FIXME: Wrap this into torch.Parameters to prevent DataParallel from crashing
        weight = next(self.parameters()).data
        
        if torch.cuda.is_available():
            return (Variable(weight.new(1, batch_size, self.hidden_size).zero_().cuda()),
                    Variable(weight.new(1, batch_size, self.hidden_size).zero_().cuda())) 
        else: 
            return (Variable(weight.new(1, batch_size, self.hidden_size).zero_()),
                    Variable(weight.new(1, batch_size, self.hidden_size).zero_())) 
    
class Decoder(nn.Module):
    """Decoder module that wraps the extended LSTM model. Being a recurrent neural network it will re-implement recurrent steps (see forward() method) and it will start from the output of the CNN encoder and the previous adaptive block to predict the likelihood scores after having looped through all the time steps of the LSTM. """

    def __init__(self, embed_size, vocab_size, hidden_size):
        super(Decoder, self).__init__()

        # Word embedding used to translate sentences into numerical vectors which will be part of the input x_t to the LSTM language model. In this case we are asking the Embedding object to plan an embedding space where up to vocab_size unique indexes should be mapped to vectors of embed_size size in R^{d / 2}.
        self.embed = nn.Embedding(vocab_size, embed_size)
        
        """
        Define an LSTM decoder to predict the hidden state h_t following Eq. (4) from the paper.

        Parameters
        ----------
        batch_first : bool [True]
            Signals the Torch module whether the first dimension of the input is always the batch size B and the second one is the sequence k (i.e., how many regions) and then the true input size of the tensor as opposed to what Torch would expect (k x B x input_size) otherwise.
        
        1 : Request one single layer of LSTM.
        """

        # LSTM decoder: x_t = [w_t; v_g] => 2 x word_embed_size
        self.LSTM = nn.LSTM(embed_size * 2, hidden_size, 1, batch_first=True)
        
        # Save hidden_size for h_t and m_t 
        self.hidden_size = hidden_size
        
        # Adaptive attention block
        self.adaptive = AdaptiveBlock(embed_size, hidden_size, vocab_size)
        
    def forward(self, V, v_g, captions, states=None):
        """Retrieve hidden and cell state for every input extracted by the word embedding of the captions recursively, and then feed them to the adaptive attention block to compute the log-likelihood scores for every word in the vocabulary.

        Parameters
        ----------
        V and v_g :
            Check previous Python docstring

        captions : torch.Tensor (B x padded_length)
            Batch of padded captions of equal length

        states : torch.Tensor [None]
            Memory cell states (if any)

        Returns
        -------
        scores, atten_weights, beta :
            Check previous Python docstring

        states : torch.Tensor
            Memory cell states (returned for Enc2Dec.sampler())
        """
        
        # Embed captions (B x padded_length x embed_size)
        embeddings = self.embed(captions)

        """
        Computation details:
            - v_g : (B x embed_size)
            - v_g.unsqueeze(1) : (B x 1 x embed_size)
            - v_g...expand_as(embeddings) : (B x padded_length x embed_size)

        These steps are necessary to make level v_g's dimensionality with embeddings', thus allowing us to use torch.cat().
        x : (B x padded_length x 2*embed_size)
        """

        # Define the input vector
        # x_t = [w_t;v_g] (as in the paper)
        x = torch.cat((embeddings, v_g.unsqueeze(1) \
                            .expand_as(embeddings)), dim=2)
        
        """
        Fill accumulators for hidden and cell memory states, h_t and m_t. They present a slightly different format in size, as this was necessary to make them compliant with the following processing on individual timesteps (x.size(1)), with CUDA, if available.

        hiddens : torch.Variable (B x padded_length x hidden_size)
            Hidden states for every input in the batch for every time step

        cells : torch.Variable (padded_length x B x hidden_size)
            Memory cell states for every every time step for every input
        """

        # TODO: Substitute with the utility func to_var()
        if torch.cuda.is_available():
            hiddens = Variable(torch.zeros(x.size(0), x.size(1), self.hidden_size).cuda())
            cells = Variable(torch.zeros(x.size(1), x.size(0), self.hidden_size).cuda())
        else:
            hiddens = Variable(torch.zeros(x.size(0), x.size(1), self.hidden_size))
            cells = Variable(torch.zeros(x.size(1), x.size(0), self.hidden_size))            
        
        # Recurrent block implementation to iteratively retrieve hidden and cell state at every timestep t, h_t and m_t. It prepares the field to the sentinel simulation at line 495 with every previous timestep, by looping through all the words in every (padded) caption, <pad> or not.
        for t in range(x.size(1)): # For each word in padded_sequence
            # x_t is the input to the LSTM at time step t
            x_t = x[:, t, :] # B x (2*embed_size)
            x_t = x_t.unsqueeze(1) # B x 1 x (2*embed_size)
            
            # See here https://pytorch.org/docs/master/generated/torch.nn.LSTM.html for details
            # Note: Remember the flag batch_first=True.
            h_t, states = self.LSTM(x_t, states) # h_t, states (c_t): B x 1 x hidden_size
            
            # Store hidden and cell state at each time step t:
            #   - Compute the next hidden state h_t
            #   - Update the current memory cell of the LSTM
            hiddens[:, t, :] = h_t.squeeze(1) # B x hidden_size
            cells[t, :, :] = states[1] # B x hidden_size
        
        cells = cells.transpose(0, 1) # B x padded_sequence x hidden_size

        # Data parallelism for adaptive attention block
        if torch.cuda.device_count() > 1:
            # IDs of available CUDA devices
            device_ids = range(torch.cuda.device_count())

            # Parallel adaptive block
            adaptive_block_parallel = nn.DataParallel(self.adaptive, device_ids=device_ids)            
            scores, atten_weights, beta = adaptive_block_parallel(x, hiddens, cells, V)
        else:
            # Sequential adaptive block
            scores, atten_weights, beta = self.adaptive(x, hiddens, cells, V)
        
        # Return states for Enc2Dec sampling purposes
        return scores, states, atten_weights, beta

class Encoder2Decoder(nn.Module): 
    """Whole KwtL architecture with a CNN images encoder and an adaptive attention-based LSTM decoder."""

    def __init__(self, embed_size, vocab_size, hidden_size):
        super(Encoder2Decoder, self).__init__()
        
        self.encoder = AttentiveCNN(embed_size, hidden_size)
        self.decoder = Decoder(embed_size, vocab_size, hidden_size)
        
    def forward(self, images, captions, lengths):
        """
        It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. 
        See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.
           
        In both cases, as seen in the paper, the CNN encoder model produces V and v_g, which are, respectively, mappings of A and A_g (single and global
        visual features) to an embedded dimension d, as it receives images as input."""

        # Data parallelism for V, v_g CNN encoder if multiple GPUs are available
        if torch.cuda.device_count() > 1:
            # IDs of available CUDA devices
            device_ids = range(torch.cuda.device_count())

            # Parallel image encoding
            encoder_parallel = torch.nn.DataParallel(self.encoder, device_ids=device_ids)
            V, v_g = encoder_parallel(images) 
        else:
            # Sequential image encoding
            V, v_g = self.encoder(images)
                                         
        # Predict scores (words log-likelihood) by decoding the sequence via language model (LSTM). A set of scores is produced for each pair (image, caption) fed in input to the Enc2Dec model from a CocoTrainLoader object, thus each image may see different outputs.
        scores, _, _,_ = self.decoder(V, v_g, captions)
        
        # Pack it to make criterion calculation more efficient
        packed_scores = pack_padded_sequence(scores, lengths,
                                             batch_first=True)
        
        return packed_scores
    
    def sampler(self, images, max_len=20):
        """Samples captions for a given batch of images
        
        Parameters
        ----------
        images : torch.Tesnor (B x 3 x 224 x 224)
            Batch of COCO's images
            
        max_len : int [20]
            Maximum length of the generated captions
            
        # TODO: Teacher forcing during training
        # TODO: Beam search to find a better sequence
        """
        
        # Data parallelism if multiple GPUs
        if torch.cuda.device_count() > 1:
            # IDs of available CUDA devices
            device_ids = range(torch.cuda.device_count())

            # Parallel image encoding
            encoder_parallel = torch.nn.DataParallel(self.encoder, device_ids=device_ids)
            V, v_g = encoder_parallel(images) 
        else:
            # Sequential image encoding
            V, v_g = self.encoder(images)

        # Build captions as a Variable object starting with the <start> token (fill_(1))
        # (B x 1) in the first iteration
        if torch.cuda.is_available():
            captions = Variable(torch.LongTensor(images.size(0), 1).fill_(1).cuda())
        else:
            captions = Variable(torch.LongTensor(images.size(0), 1).fill_(1))
        
        # Define tracker variables
        sampled_ids = [] # Chosen word indexes list
        attention = []   # Attention weights per-word
        Beta = []        # Sentinel score per-word

        # Track the LSTN decoder memory at each step
        # and update this variable accordingly
        states = None

        for _ in range(max_len): # Sample one word per iteration

            scores, states, atten_weights, beta = self.decoder(V, v_g, captions, states) # scores: B x vocab_size x 1

            # Get best predicted word index
            predicted = scores.max(2)[1] # [1] := argmax --> predicted: B x 1
            captions  = predicted
            
            # Update tracker variables
            sampled_ids.append(captions)
            attention.append(atten_weights)
            Beta.append(beta)
        
        sampled_ids = torch.cat(sampled_ids, dim=1) # B x max_lem
        attention = torch.cat(attention, dim=1)     # B x max_len x k
        Beta = torch.cat(Beta, dim=1)               # B x max_len
        
        return sampled_ids, attention, Beta
